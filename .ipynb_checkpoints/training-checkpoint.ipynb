{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-2zQp7FwyVW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc , logfbank\n",
    "import librosa as lr\n",
    "import os, glob, pickle\n",
    "import librosa\n",
    "from scipy import signal\n",
    "import noisereduce as nr\n",
    "from glob import glob\n",
    "import librosa\n",
    "%matplotlib inline\n",
    "#All the Required Packages and Libraies are installed.\n",
    "import soundfile\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D, Flatten, LSTM\n",
    "from keras.layers import Dropout,Dense,TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "crJksogi_leT",
    "outputId": "2d951a73-5110-40c1-88c1-f712213a2a4a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def getListOfFiles(dirName):\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = []\n",
    "    for entry in listOfFile:\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles += getListOfFiles(fullPath)\n",
    "        elif fullPath.endswith('.wav'):\n",
    "            allFiles.append(fullPath)\n",
    "    return allFiles\n",
    "\n",
    "dirName = r'C:\\Users\\rishi\\ml_project\\my_audio_project\\speech-emotion-recognition-ravdess-data'\n",
    "listOfFiles = getListOfFiles(dirName)\n",
    "\n",
    "print(\"Total .wav files:\", len(listOfFiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBXrNYnqX5I2",
    "outputId": "8fba7775-7961-45e4-b1cb-f2ee1484bc1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can you talking by the\n",
      "can you talking by the do\n",
      "talk to sitting by\n",
      "talk to Siri by the door\n",
      "kids talking by the do\n",
      "kids talking by the\n",
      "talk to sitting\n",
      "dogs are sitting by\n",
      "can you talking\n",
      "kids are talking by\n",
      "dogs are sitting by\n",
      "dogs are sitting by\n",
      "can you talking by the door\n",
      "can you talking by the door\n",
      "dogs are sitting by the\n",
      "error\n",
      "kids talking by the door\n",
      "kids are talking\n",
      "dogs are sitting by the\n",
      "dogs are sitting by the\n",
      "kids are talking by the\n",
      "error\n",
      "talk just sitting by the\n",
      "talk to sitting by\n",
      "kids are talking by the\n",
      "kids are talking by the door\n",
      "dogs are sitting by the\n",
      "dogs are sitting by the\n",
      "kids are talking by the door\n",
      "kids are talking by the door\n",
      "dogs are sitting by\n",
      "dogs are sitting by the\n",
      "Shin-chan talking by the door\n",
      "screenshot talking by\n",
      "dogs are sitting by the\n",
      "dogs are sitting by the\n",
      "kids are talking by the\n",
      "kids talking by the\n",
      "dogs are sitting by the door\n",
      "dogs are sitting by the\n",
      "talking by the door\n",
      "kids are talking by\n",
      "dogs are sitting by the door\n",
      "stocks are sitting by\n",
      "kids talking by the door\n",
      "kids talking by the door\n",
      "doctor sitting by the door\n",
      "talk just sitting by\n",
      "kids are talking by the door\n",
      "is it talking about\n",
      "dogs sitting by the door\n",
      "dogs are sitting by the do\n",
      "can you talking by the door\n",
      "can you talking by the\n",
      "dogs are sitting by\n",
      "talk just sitting by the\n",
      "kids are talking by the door\n",
      "kids are talking by the door\n",
      "dogs are sitting by the\n",
      "talk just sitting by the\n",
      "kids are talking by\n",
      "kids are talking by the\n",
      "dogs are sitting by\n",
      "dogs are sitting by the\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "r=sr.Recognizer()\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "    with sr.AudioFile(listOfFiles[file]) as source:\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print(text)\n",
    "        except:\n",
    "            print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5etZWDtdEzwP",
    "outputId": "eb298829-e3f5-4b2b-9cc6-0cf931c60e71"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "dirName = r'C:\\Users\\rishi\\ml_project\\my_audio_project\\speech-emotion-recognition-ravdess-data'\n",
    "# Print the full directory listing to check if the folder exists.\n",
    "if os.path.isdir(dirName):\n",
    "    print(f\"Found directory: {dirName}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {dirName}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Cleaning Step is Performed where:\n",
    "#DOWN SAMPLING OF AUDIO FILES IS DONE  AND PUT MASK OVER IT AND DIRECT INTO CLEAN FOLDER\n",
    "#MASK IS TO REMOVE UNNECESSARY EMPTY VOICES AROUND THE MAIN AUDIO VOICE \n",
    "def envelope(y , rate, threshold):\n",
    "    mask=[]\n",
    "    y=pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8bjtuN2wZFFy",
    "outputId": "1120c86c-3ec6-431a-e562-444cba4ab3e3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=wavfile.WavFileWarning)\n",
    "\n",
    "#Plotting the Basic Graphs for understanding of Audio Files :\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "    audio , sfreq = lr.load(listOfFiles[file])\n",
    "    time = np.arange(0 , len(audio)) / sfreq\n",
    "    \n",
    "    fig ,ax = plt.subplots()\n",
    "    ax.plot(time , audio)\n",
    "    ax.set(xlabel = 'Time (s)' , ylabel = 'Sound Amplitude')\n",
    "    plt.show()\n",
    "    \n",
    "#PLOT THE SEPCTOGRAM\n",
    "for file_path in listOfFiles[:10]:\n",
    "    sample_rate, samples = wavfile.read(file_path)\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if samples.ndim > 1:\n",
    "        samples = samples.mean(axis=1)\n",
    "\n",
    "    frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.pcolormesh(times, frequencies, spectrogram, shading='gouraud', cmap='viridis')\n",
    "    plt.colorbar(label='Intensity [dB]')\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blzilE39fZxk"
   },
   "outputs": [],
   "source": [
    "#Next Step is In-Depth Visualisation of Audio Fiels and its certain features to plot for.\n",
    "#They are the Plotting Functions to be called later.\n",
    "def plot_signals(signals):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Time Series' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(signals.keys())[i])\n",
    "            axes[x,y].plot(list(signals.values())[i])\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def plot_fft(fft):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Fourier Transform' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            data = list(fft.values())[i]\n",
    "            Y,freq = data[0] , data[1]\n",
    "            axes[x,y].set_title(list(fft.keys())[i])\n",
    "            axes[x,y].plot(freq , Y)\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def plot_fbank(fbank):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Filter Bank Coefficients' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(fbank.keys())[i])\n",
    "            axes[x,y].imshow(list(fbank.values())[i],cmap='hot', interpolation = 'nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def plot_mfccs(mfccs):\n",
    "    fig , axes = plt.subplots(nrows=2, ncols=5,sharex =False , sharey=True, figsize=(20,5))\n",
    "    fig.suptitle('Mel Frequency Capstrum  Coefficients' , size=16)\n",
    "    i=0\n",
    "    for x in range(2):\n",
    "        for y in range(5):\n",
    "            axes[x,y].set_title(list(mfccs.keys())[i])\n",
    "            axes[x,y].imshow(list(mfccs.values())[i],\n",
    "                             cmap='hot', interpolation = 'nearest')\n",
    "            axes[x,y].get_xaxis().set_visible(False)\n",
    "            axes[x,y].get_yaxis().set_visible(False)\n",
    "            i +=1\n",
    "\n",
    "def calc_fft(y,rate):\n",
    "    n = len(y)\n",
    "    freq = np.fft.rfftfreq(n , d= 1/rate)\n",
    "    Y= abs(np.fft.rfft(y)/n)\n",
    "    return(Y,freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YS6g-q2YrWad",
    "outputId": "b2d3de0d-2f41-47e0-f6d2-1e51dcd62049"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here The Data Set is loaded and plots are Visualised by Calling the Plotting Functions .\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "import numpy as np\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "    rate, data = wav.read(listOfFiles[file])\n",
    "    fft_out = fft(data)\n",
    "    %matplotlib inline\n",
    "    plt.plot(data, np.abs(fft_out))\n",
    "    plt.show()\n",
    "\n",
    "signals={}\n",
    "fft={}\n",
    "fbank={}\n",
    "mfccs={}\n",
    "# load data\n",
    "for file in range(0 , len(listOfFiles) , 1):\n",
    "#     rate, data = wavfile.read(listOfFiles[file])\n",
    "     signal,rate =librosa.load(listOfFiles[file] , sr=44100)\n",
    "     mask = envelope(signal , rate , 0.0005)\n",
    "     signals[file] = signal\n",
    "     fft[file] = calc_fft(signal , rate)\n",
    "\n",
    "     bank = logfbank(signal[:rate] , rate , nfilt = 26, nfft = 1103).T\n",
    "     fbank[file] = bank\n",
    "     mel = mfcc(signal[:rate] , rate , numcep =13 , nfilt = 26 , nfft=1103).T\n",
    "     mfccs[file]=mel\n",
    "\n",
    "plot_signals(signals)\n",
    "plt.show()\n",
    "\n",
    "plot_fft(fft)\n",
    "plt.show()\n",
    "\n",
    "plot_fbank(fbank)\n",
    "plt.show()\n",
    "\n",
    "plot_mfccs(mfccs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5B4iB7Dq-BUc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import os, glob\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "\n",
    "output_dir = r'C:\\Users\\rishi\\clean_speech'\n",
    "os.makedirs(output_dir, exist_ok=True)  # ✅ Create the folder if it doesn't exist\n",
    "\n",
    "for file in tqdm(glob.glob(r'C:\\Users\\rishi\\speech-emotion-recognition-ravdess-data\\\\**\\\\*.wav')):\n",
    "    file_name = os.path.basename(file)\n",
    "    signal, rate = librosa.load(file, sr=16000)\n",
    "    mask = envelope(signal, rate, 0.0005)\n",
    "    wavfile.write(filename= r'C:\\Users\\rishi\\clean_speech\\\\'+str(file_name), rate=rate,data=signal[mask])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction of Audio Files Function \n",
    "#Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions in the RAVDESS dataset to be classified Audio Files based on . \n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "#These are the emotions User wants to observe more :\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data and extract features for each sound file\n",
    "from glob import glob\n",
    "import os\n",
    "import glob\n",
    "def load_data(test_size=0.33):\n",
    "    x,y=[],[]\n",
    "    answer = 0\n",
    "    for file in glob.glob(r'C:\\Users\\rishi\\clean_speech\\\\*.wav'):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        if emotion not in observed_emotions:\n",
    "            answer += 1\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append([emotion,file_name])\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n",
    "x_train, x_test, y_train, y_test = load_data()\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")\n",
    "print(f\"Example label: {y_train[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "import librosa\n",
    "import numpy as np\n",
    "x_train,x_test,y_trai,y_tes=load_data(test_size=0.25)\n",
    "print(np.shape(x_train),np.shape(x_test), np.shape(y_trai),np.shape(y_tes))\n",
    "y_test_map = np.array(y_tes).T\n",
    "y_test = y_test_map[0]\n",
    "test_filename = y_test_map[1]\n",
    "y_train_map = np.array(y_trai).T\n",
    "y_train = y_train_map[0]\n",
    "train_filename = y_train_map[1]\n",
    "print(np.shape(y_train),np.shape(y_test))\n",
    "print(*test_filename,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the shape of the training and testing datasets\n",
    "# print((x_train.shape[0], x_test.shape[0]))\n",
    "print((x_train[0], x_test[0]))\n",
    "#Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE MODEL\n",
    "import pickle\n",
    "# Save the Modle to file in the current working directory\n",
    "#For any new testing data other than the data in dataset\n",
    "\n",
    "Pkl_Filename = \"Emotion_Voice_Detection_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Emotion_Voice_Detection_Model = pickle.load(file)\n",
    "\n",
    "Emotion_Voice_Detection_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#predicting :\n",
    "y_pred=Emotion_Voice_Detection_Model.predict(x_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the Prediction probabilities into CSV file \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "y_pred1 = pd.DataFrame(y_pred, columns=['predictions'])\n",
    "y_pred1['file_names'] = test_filename\n",
    "print(y_pred1)\n",
    "y_pred1.to_csv('predictionfinal.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RECORDED USING MICROPHONE:\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024 \n",
    "FORMAT = pyaudio.paInt16 #paInt8\n",
    "CHANNELS = 2 \n",
    "RATE = 44100 #sample rate\n",
    "RECORD_SECONDS = 4\n",
    "WAVE_OUTPUT_FILENAME = \"output10.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK) #buffer\n",
    "\n",
    "print(\"* recording\")\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data) # 2 bytes(16 bits) per channel\n",
    "\n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"output10.wav\"):\n",
    "    print(\"✅ File was created at:\", os.path.abspath(\"output10.wav\"))\n",
    "else:\n",
    "    print(\"❌ File was NOT created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The file 'output10.wav' in the next cell is the file that was recorded live using the code :\n",
    "data, sampling_rate = librosa.load('output10.wav')\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa.display\n",
    "import glob \n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveshow(data, sr=sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Check basic audio properties\n",
    "try:\n",
    "    y, sr = librosa.load('output10.wav', sr=None)\n",
    "    print(f\"Duration: {len(y)/sr:.2f} seconds\")\n",
    "    print(f\"Sample rate: {sr} Hz\")\n",
    "    \n",
    "    if len(y) < 2048:  # Minimum for processing\n",
    "        print(\"Error: Audio too short (minimum 2048 samples needed)\")\n",
    "    else:\n",
    "        print(\"Audio appears valid for processing\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pickle\n",
    "\n",
    "def extract_feature_fixed(file_path):\n",
    "    \"\"\"Guaranteed to return 180 features matching your training setup\"\"\"\n",
    "    try:\n",
    "        # 1. Load audio with consistent parameters\n",
    "        y, sr = librosa.load(file_path, sr=22050)  # Force resample to 22.05kHz\n",
    "        \n",
    "        # 2. Pad if too short (minimum 2048 samples ~93ms at 22.05kHz)\n",
    "        if len(y) < 2048:\n",
    "            y = np.pad(y, (0, 2048 - len(y)), mode='constant')\n",
    "        \n",
    "        # 3. Extract EXACTLY the same features as training\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)  # 40\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)      # 12 \n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)      # 128\n",
    "        \n",
    "        # 4. Combine (40 + 12 + 128 = 180 features)\n",
    "        features = np.concatenate([mfccs, chroma, mel]).reshape(1, -1)\n",
    "        \n",
    "        print(\"Successfully extracted features:\", features.shape)\n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    with open('Emotion_Voice_Detection_Model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print(f\"Model loaded. Expects {model.n_features_in_} features\")\n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {str(e)}\")\n",
    "    model = None\n",
    "\n",
    "# Process and predict\n",
    "if model:\n",
    "    features = extract_feature_fixed('output10.wav')\n",
    "    \n",
    "    if features is not None:\n",
    "        try:\n",
    "            # Final validation\n",
    "            assert features.shape[1] == model.n_features_in_, \\\n",
    "                   f\"Got {features.shape[1]} features, expected {model.n_features_in_}\"\n",
    "            \n",
    "            # Predict\n",
    "            emotion = model.predict(features)[0]\n",
    "            print(\"\\nFinal Prediction:\", emotion)\n",
    "            \n",
    "            # Confidence scores\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                print(\"\\nConfidence:\")\n",
    "                for emo, prob in zip(model.classes_, model.predict_proba(features)[0]):\n",
    "                    print(f\"{emo}: {prob:.1%}\")\n",
    "                    \n",
    "        except AssertionError as e:\n",
    "            print(\"Dimension Error:\", str(e))\n",
    "        except Exception as e:\n",
    "            print(\"Prediction failed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
